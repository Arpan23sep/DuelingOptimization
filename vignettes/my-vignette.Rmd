---
title: "my-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(DuelingOptimization)
```

# DuelingOptimization

DuelingOptimization is an R package that implements algorithms for convex optimization with preference-based (dueling) feedback, based on the paper "Dueling Convex Optimization" by Saha et al. This package provides tools for performing convex optimization using only noisy binary feedback on pairs of decision points, as opposed to direct gradient or function value information. The algorithms are suitable for applications like recommender systems, ranking, and online learning, where preference feedback is commonly available.

#### Overview 

This package includes two primary functions:

beta_Normalized_Gradient: Implements a normalized gradient descent algorithm for general β-smooth convex functions using binary feedback. alpha_beta_NGD: Extends the gradient descent to strongly convex and β-smooth functions, achieving faster convergence. The core of these algorithms lies in estimating gradient directions based on noisy, preference-based comparisons. This enables optimization in scenarios where only binary feedback is available for queried pairs of points.

Mathamatical problem: This work addresses the challenge of **convex optimization with preference (dueling) feedback**, where the learner only observes noisy binary feedback (win-loss) comparing pairs of queried points based on their function values. Unlike traditional optimization, this setting lacks even zeroth-order feedback, making it particularly relevant in real-world applications like recommender systems or customer preference learning, where only binary preference data is available.

The focus is on **online convex optimization (OCO)**, aiming to find a near-optimal point with minimal query complexity by actively querying noisy {0, 1} comparisons between decision point pairs. The study proves an impossibility result for the **non-stationary OCO setting**, where the convex function changes over time, showing that achieving this objective is not feasible in such scenarios.

For the **stationary OCO problem**, where the function remains constant, the authors propose a **normalized gradient descent algorithm** to find an ϵ\epsilonϵ-optimal point. Key contributions include:

Objective:

Noiseless Case: Normalized-Gradient Descent

F is beta smooth:

Algorithm:

Convergence Guarantee:

Example1: Logistic Regression

Beta_NGD optimum:

Example2:Linear Regression

Example 3: Dimension Effects Plot

F is alpha strongly convex & beta smooth:

Algorithm:

Convergence Guarnatee:

General Case:Noisy sign feedback

Sign_recovery:

Algorithm:

Our Extension:

PL inequality Case:

General Non_Convex Setup:

Neural Network Optimization:

Conclusion:

Reference:
