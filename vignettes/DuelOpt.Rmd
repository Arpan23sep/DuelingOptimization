---
title: "Learning from Comparisons: A Journey Through DuelingOptimization"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{algorithm}
  - \usepackage{algorithmic}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(DuelingOptimization)
```

# DuelingOptimization

DuelingOptimization is an R package designed to implement algorithms for convex optimization with preference-based (dueling) feedback, based on the paper Dueling Convex Optimization by Saha et al. This package provides tools to solve optimization problems using only noisy binary feedback on pairs of decision points, instead of relying on direct gradients or function values.

The package is particularly suited for applications such as recommender systems, ranking, and online learning, where preference-based feedback is commonly available.

### Overview

This package includes four primary functions:

beta_NGD: Implements a normalized gradient descent algorithm for general
β-smooth convex functions using binary feedback.

beta_NGD_optimum: Computes the parameters for normalized gradient
descent based on the strong convexity and smoothness properties of the
objective function.

alpha_beta_NGD: Extends the gradient descent to strongly convex and
β-smooth functions, achieving faster convergence.

signRecovery: Performs sign recovery by repeatedly comparing (resampling) two points using a specified objective function. The implementation of this function utilizes RCPP for improved performance.

### Mathematical problem

This work addresses the challenge of convex optimization with preference
(dueling) feedback, where the learner only observes noisy binary
feedback (win-loss or 0/1) comparing pairs of queried points based on
their function values. Unlike traditional optimization, this setting
lacks even zeroth-order feedback, making it particularly relevant in
real-world applications like recommender systems and customer preference
learning, where only binary preference data is available.

The focus is on **online convex optimization (OCO)**, aiming to find a
near-optimal point with minimal query complexity by actively querying
noisy {0, 1} comparisons between decision point pairs. The study demonstrates
an impossibility result for the non-stationary OCO setting, where the
convex function changes over time, showing that achieving this objective
is not feasible in such scenarios.

For the stationary OCO problem, where the function remains constant over
time, the authors propose a **normalized gradient descent algorithm** to
find an ϵ\epsilonϵ-optimal point.

### Objective

The objective of this problem is to address the standard
$(\epsilon, \delta)$-probably approximately correct (PAC) function
optimization problem. Specifically, given any fixed
$\epsilon, \delta \in (0, 1)$, the goal is to find a decision point
$x \in \mathbb{R}^d$ with the minimum possible pairwise-query complexity
$\{(x_t, y_t)\}_{t=1}^T$ such that the final point $x$ satisfies:

$$
\Pr\left(\bar{f}(x) - \bar{f}(x^*) < \epsilon\right) \geq 1 - \delta,
$$

where

$$
\bar{f}(x) := \frac{1}{T} \sum_{t=1}^T \mathbb{E}_{f_t \sim P}[f_t(x)],
$$

for all $x \in \mathbb{R}^d$, represents the average function value, and

$$
x^* := \arg \min_{z \in \mathbb{R}^d} \bar{f}(z)
$$

denotes the global minimizer of $\bar{f}$.

This package provides efficient algorithms to achieve this objective by
leveraging dueling feedback (pairwise comparisons) and iterative
optimization techniques.

### Noiseless Case

Here we will analyze the case of "no-noise" i.e upon querying any duel
(x,y) the learner gets access to its true sign feedback
$$ 1 (f(x) \geq f(y))$$.Let us first look of our main algorithm for beta
smooth convex optimization which is based on technique of normalized
gradient descent.

### Function is beta smooth

A differentiable function $f: \mathbb{R}^d \to \mathbb{R}$ is said to be
beta-smooth (or $\beta$-smooth, where $\beta > 0$) if its gradient
$\nabla f(x)$ is Lipschitz continuous. Mathematically, this means there
exists a constant $\beta > 0$ such that for all $x, y \in \mathbb{R}^d$:
$$
\|\nabla f(x) - \nabla f(y)\|_2 \leq \beta \|x - y\|_2.
$$


#### Convergence Guarantee

This theorem ensures that our algorithm will converge. Suppose beta_NGD is run with $$
\eta = \frac{\sqrt{\epsilon}}{20\sqrt{d\beta}}, \quad 
\gamma = \frac{(\epsilon/\beta)^{3/2}}{240\sqrt{2}d(D + \eta T)^2\sqrt{\log \frac{480\sqrt{\beta}d(D + \eta T)}{\sqrt{2}\epsilon}}}, \quad 
\text{and} \quad T_\epsilon = O\left(\frac{d\beta D}{\epsilon}\right),
$$ where $D \geq \|\mathbf{x}_1 - \mathbf{x}^*\|_2$ (an assumed known
upper bound).

Then beta_NGD returns $$
\mathbb{E}[f(\tilde{\mathbf{x}}_{T+1})] - f(\mathbf{x}^*) \leq \epsilon
$$ with sample complexity $2T_\epsilon$.

#### Getting Started (beta_NGD): A Simple Example 

```{r}
# Define an initial starting point and initialise the parameters
f <- function(x) sum(x^2)    #Define the smooth convex function
initial_point <- 2           #Give an initial starting point
eta <- 0.01                  #controls the update size at each iteration
gamma <- 0.1                 #perturbation parameter
T <- 1000                    #no of iterations

# Run the normalized gradient descent
optimal_point <- beta_NGD(initial_point, eta, gamma, T,f)
#beta_NGD will return a list that contains optimal point after T iterations 
#and a vector containing the best function value observed at each iteration
print(optimal_point$optimum)           #printing the optimum value 
```

#### Observation

The optimal value obtained is 0, as expected. This aligns with the theoretical minimum of the squared function, which is 0.

### Multiple Linear Regression using beta_NGD function

In this section, we apply the beta_NGD function to perform linear regression on the case described above. The results demonstrate the algorithm's ability to effectively minimize the objective function in a regression setting.

```{r,fig.width=7, fig.height=4}
#Linear Regression using beta_NGD 
# Set random seed for reproducibility
  set.seed(42)

# Generating data
 n_samples <- 100                              #No of samples
 I <- rep(1, n_samples)                        # Intercept term(vector of 1's)
 X1 <- runif(n_samples, 0, 1)                  # Feature 1
 X2 <- runif(n_samples, 0, 0.5)                # Feature 2
 X3 <- runif(n_samples, 0, 0.8)                # Feature 3
 X <- cbind(I, X1, X2, X3)
 beta <- c(5, 1, 2, 1)                         # True coefficients
 # Response variable with noise
 y <- X %*% beta + rnorm(n_samples, mean = 0, sd = 0.4)
 data <- data.frame(X1 = X1, X2 = X2, X3 = X3, y = y)

 # Fit a linear regression model using lm
 model <- lm(y ~ ., data = data)
 print("lm output")
 print(coef(model))

 # Define the objective function (Residual/error Sum of Squares)
 f_value <- function(beta) {
   sum((y - X %*% beta)^2)
 }
# Apply Beta-NGD Optimization
result <- beta_NGD(
  c(4, 2, 3, 2),                          #initial point
  c(0.003, 0.007, 0.005, 0.007),          #eta
  c(0.001, 0.002, 0.003, 0.004),          #Gamma
  5000,                                   #no of iterations
  f_value                                 #Linear regression objective function 
 )

 f_array <- result$f_array               #Extracting output
 optimum <- result$optimum
 print("Beta_NGD result")
 print(optimum)
 # Plot the RSS over iterations
 library(ggplot2)
 f_data <- data.frame(
   Index = 0:(length(f_array) - 1),
   Value = f_array
 )
 ggplot(f_data, aes(x = Index, y = Value)) +
   geom_line(color = "blue") +          
   labs(title = "RSS vs Iterations",
        x = "Iterations",
        y = "RSS") +
   theme_minimal() +
   theme(plot.title = element_text(hjust = 0.5))  
```

#### Observation

The results obtained using the lm function and our algorithm are remarkably similar. Notably, our algorithm achieves these results without relying on any underlying model assumptions.

### Beta_NGD_optimum

This algorithm begins with fixed initial values derived from the characteristics of the function. Theoretical results guarantee that this algorithm converges significantly faster compared to the previous method, which relied on random initialization.

#### Getting Started: A Simple Example

```{r}
f <- function(x) sum(x^2)          #Defining the same norm function

initial_point <- 1.5               #Initial point to start

D <- 10                           #Distance between optimal point and initial point 
                                  #Start with large values as optimal is unknown to user 

eigen_max <- 2                    #Max eigenvalue of the hessian matrix
                                  #Double derivative of x^2 gives 2
#Run our algorithm with these initial values
beta_NGD_optimal_point <- beta_NGD_optimum (initial_point, D, eigen_max, epsilon = 0.1, f)
print(beta_NGD_optimal_point$optimum)
```

#### Observations

When analyzing the array of objective function values over iterations, it becomes evident that beta_NGD_optimum converges more rapidly to the optimal point compared to the previous algorithm. This highlights the efficiency of the initialization based on theoretical insights.

### Multiple Logistic Regression using beta_NGD_optimum
In this section, we demonstrate the use of the beta_NGD_optimum function for logistic regression. The algorithm optimizes the logistic loss function for a synthetic dataset, leveraging theoretical insights to ensure faster convergence. This approach highlights its applicability to classification problems.

```{r,fig.width=7, fig.height=4}
 # Logistic regression with Beta_NGD_optimum
 # Set random seed for reproducibility
 set.seed(213)

 # Generating data for logistic regression
 n <- 1000  # Number of data points
 d <- 5     # Number of features
 X <- matrix(rnorm(n * d), n, d)  # random features
 # Generate binary labels with a true beta
 true_beta <- c(1, 2, 3, 4, 5)
 prob <- 1 / (1 + exp(-X %*% true_beta))
 y <- rbinom(n, 1, prob)  # Binary labels
 # Define the logistic loss function as the objective function
 f_value <- function(beta) {
   linear_comb <- X %*% beta
   loss <- -mean(y * log(1 / (1 + exp(-linear_comb))) +
                 (1 - y) * log(1 - 1 / (1 + exp(-linear_comb))))
   return(loss)
 }

 # Initialize parameters
 initial_point <- c(2, 1, 4, 3, 6)
 D <- 10     
 Hessian <- t(X) %*% X               
 eigenvalues <- eigen(Hessian)$values
 eigen_max <- 0.25 * max(eigenvalues)      #maximum eigenvalues of hessian matrix

 # Run the beta_NGD_optimum function 
 result <- beta_NGD_optimum(initial_point, D, eigen_max, epsilon = 0.1, f_value)
 point <- result$optimum
 f_array <- result$f_array
 print("Estimated parameters using beta_NGD_optimum:")
 print(point)
 # Compare with logistic regression solution from glm for verification
 glm_fit <- glm(y ~ X - 1, family = binomial)  # Fit logistic regression without intercept
 print("GLM estimated beta:")
 print(coef(glm_fit))

 # Create a data frame for visualization
 f_data <- data.frame(
   Index = 0:(length(f_array) - 1),
   Value = f_array
 )

 # Plot negative log-likelihood vs iterations using ggplot2
 library(ggplot2)
 ggplot(f_data, aes(x = Index, y = Value)) +
   geom_line(color = "blue") +          
   labs(title = "Negative log-likelihood vs Iterations",
        x = "Iterations",
        y = "Negative log-likelihood") +
   theme_minimal() +
   theme(plot.title = element_text(hjust = 0.5))  

```

#### Observation

The outputs from the lm function and our algorithm are remarkably similar, demonstrating the accuracy of our approach. Additionally, the algorithm exhibits fast convergence, highlighting the efficiency of the theoretical initialization strategy.

### α-Strongly Convex and β-Smooth Functions

We demonstrate an improved convergence rate for functions that are both
**α-strongly convex** and **β-smooth**. In this scenario, we leverage
β-NGD, which is optimal for β-smooth convex functions, as a black-box
subroutine to develop an efficient algorithm for this specialized
setting.

Our proposed method, **(α, β)-NGD** , follows a **phase-wise iterative
optimization** approach. Within each phase, beta_NGD is employed as a
black box to identify an $\epsilon_k$-optimal point, where $\epsilon_k$
decreases exponentially across phases (beginning with $\epsilon_1 = 1$).
The optimization process for the $(k+1)$-th phase is **warm-started**
using the optimizer returned by the previous phase.

This iterative framework ensures faster convergence by combining the
strong convexity property of the objective function with the efficient
gradient descent method for β-smooth functions, making it suitable for
scenarios requiring both precision and efficiency.

#### Norm Function Example

The simple norm calculation of a vector serves as an objective function that is both α-strongly convex and β-smooth. As such, our algorithm is well-suited for this task. Below, we demonstrate the application of our algorithm to this example.

```{r}
 # norm function
 f <- function(x) sum(x^2)  
                            
 # Set parameters
 initial_point <- c(1, 1)
 alpha <- 0.1
 beta <- 1.0
 D <- 3
 tolerance <- 0.01

 # Run the algorithm
 result <- alpha_beta_NGD(initial_point, alpha, beta, D, tolerance, f)
 print(result)
```

#### Observation

The optimal value is close to 0, as expected, but this time, the convergence is achieved much faster compared to previous methods, highlighting the efficiency of the (α, β)-NGD algorithm.

#### Convergence Guarantee

Consider $f$ to be **α-strongly convex** and **β-smooth**. Using
$(\alpha, \beta)$-NGD, we achieve an improved convergence rate.
Specifically, this algorithm ensures:

$$
\mathbb{E}[f(\tilde{x})] - f(x^*) \leq \epsilon,
$$

with a sample complexity (number of pairwise comparisons) of:

$$
O\left(d\frac{\beta}{\alpha}\left(\log_2\frac{\alpha}{\epsilon} + \|x_1 - x^*\|^2\right)\right),
$$

where: -

-   $d$: Dimension of the problem

-    $\beta$: Smoothness constant

-    $\alpha$: Strong convexity constant

-    $\epsilon$: Desired optimization accuracy

-    $\|x_1 - x^*\|^2$: Squared distance between the initial point $x_1$
    and the optimal point $x^*$.

This result demonstrates that the $(\alpha, \beta)$-NGD algorithm
leverages the strong convexity property to **significantly** reduce the
number of pairwise comparisons required for achieving a given level of
accuracy.

#### Effects of Linear Regression as the Number of Features (Dimension d) Increases

We analyze the effect of increasing the number of features (d) on our linear regression algorithm's performance. The following plot illustrates the behavior of the algorithm for dimensions $$d=2,3,4,5$$. By observing the trends, we aim to understand how the complexity of the model evolves as the dimensionality increases.



```{r,fig.width=7, fig.height=6}
library(ggplot2)

set.seed(345)
epsilon <- 0.1       #Fixing error bound

# Function to calculate RSS
RSS <- function(beta, X, y) {
  return(sum((y - X %*% beta)^2))  # Residual Sum of Squares (RSS)
}


# Generate data for multiple scenarios of beta length or dimension
n_samples <- 50
I <- rep(1, n_samples)
scenarios <- list(
  "2 features" = c(5, 1),
  "3 features" = c(5, 1, 2),
  "4 features" = c(5, 1, 2, 1),
  "5 features" = c(5, 1, 2, 1, 3),
  "6 features" = c(5, 1, 2, 1, 3, 4)
)
initials <- list(
  "2 features" = c(4, 2),
  "3 features" = c(4, 2, 3),
  "4 features" = c(4, 2, 3, 2),
  "5 features" = c(4, 2, 3, 2, 2),
  "6 features" = c(4, 2, 3, 2, 2, 5)
)
X_list <- list(
  "2 features" = cbind(I, replicate(1, runif(n_samples, 0, 1))),
  "3 features" = cbind(I, replicate(2, runif(n_samples, 0, 1))),
  "4 features" = cbind(I, replicate(3, runif(n_samples, 0, 1))),
  "5 features" = cbind(I, replicate(4, runif(n_samples, 0, 1))),
  "6 features" = cbind(I, replicate(5, runif(n_samples, 0, 1)))
)
T_max=0
f_data_list <- list()
#initialsing the parameter
for (scenario in names(scenarios)) {
  X <- X_list[[scenario]]
  beta <- scenarios[[scenario]]
  initial_point <- initials[[scenario]]
  d<- length(initial_point)
  D= sum((beta-initial_point)^2)+1
  eigen_max= max(eigen(2 * t(X) %*% X)$values)
  T= d*eigen_max*D/epsilon
  if (T> T_max){T_max=T}
}
for (scenario in names(scenarios)) {
  beta <- scenarios[[scenario]]
  n_features <- length(beta)
  
  # Generate synthetic data
  X <- X_list[[scenario]]
  #Generating response y
  y <- X %*% beta + rnorm(n_samples, mean = 0, sd = 0.4)
  
  # Compute RSS values
  initial_point= initials[[scenario]]
  d= length(initial_point)
  B= max(eigen(2 * t(X) %*% X)$values)
  eta= sqrt(epsilon)/(20*sqrt(d*B))
  D= sum((beta-initial_point)^2)+1
  T= T_max
  gamma = (epsilon / B)^(3 / 2) / (240 * sqrt(2) * d * (D + eta * T)^2 * 
         sqrt(log(480 * sqrt(B * d) * (D + eta * T) / sqrt(2 * epsilon))))

  f= function(beta){return(RSS(beta, X, y))}
  f_array <- beta_NGD(initial_point, eta, gamma, T, f)$f_array
  
  # Add to combined data frame
  f_data <- data.frame(
    Iteration = 0:(length(f_array) - 1),
    RSS = f_array,
    Scenario = scenario
  )
  f_data_list[[scenario]] <- f_data
  print(paste0("Completed ", scenario))
}

# Combine all data
f_data_combined <- do.call(rbind, f_data_list)

# Plotting
print(ggplot(f_data_combined, aes(x = Iteration, y = RSS, color = Scenario)) +
        geom_line() +
        labs(title = "RSS vs Iterations for Different Beta Lengths or dimension",
             x = "Iterations",
             y = "RSS",
             color = "Scenario") +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5)))

```

#### Observations

From the plot, we can observe that as the number of features (d) increases, the convergence of the algorithm becomes slower. This is expected because higher dimensions introduce more complexity into the optimization process, requiring more iterations for the algorithm to minimize the residual sum of squares (RSS). However, despite this increased complexity, the algorithm still demonstrates robust convergence behavior across all feature dimensions. This suggests that the algorithm efficiently handles high-dimensional data while maintaining performance.


### General Case:Noisy sign feedback

In many real-world scenarios, comparison feedback may be noisy.
Specifically, the oracle provides feedback such that:

$$
P(o_t = 1 \mid f_t(y_t) > f_t(x_t)) = \frac{1}{2} + \nu,
$$

for some $\nu \in (0, 0.5]$.

The algorithms proposed earlier rely on access to the true sign
feedback:

$$
o_t = 1(f(x_t) > f(y_t)),
$$

for every pairwise query $(x_t, y_t)$. However, noisy feedback may
introduce incorrect signs where the oracle fails.

#### Resampling Trick

To address this issue, the paper introduces a 'resampling-trick'. This
approach effectively mitigates the impact of noise in the feedback by
ensuring more reliable comparisons through repeated queries. The
algorithm performs multiple resamples for each pairwise query and
aggregates the feedback to estimate the true sign, reducing the impact
of incorrect comparisons caused by noise. We use our this signRecovery
function for this.

### Getting Started: A Simple Example

```{r}
# Defining a function
f <- function(x) sum(x^4-x)
# Perform sign recovery
x <- c(1,2)         #first point
y <- c(3,4)         #second point
delta <- 0.01       #accuracy bound
signRecovery(x , y , delta , f = f)
```

#### Observation 
The signRecovery function successfully returns the true sign as expected by leveraging the resampling process multiple times. This demonstrates its effectiveness in mitigating noise and ensuring reliable outcomes in the optimization process.

### Our Extensions

We extend our approach to demonstrate its applicability to a broader class of functions that satisfy the well-known Polyak-Łojasiewicz (PL) inequality. This extension ensures faster convergence and applicability to non-convex scenarios where the PL inequality holds, thereby broadening the utility of our optimization algorithms.

#### Polyak-Łojasiewicz (PL) Inequality

A function $f$ satisfies the **Polyak-Łojasiewicz (PL) inequality** if
there exists a positive constant $\mu > 0$ such that:

$$
\frac{1}{2} \|\nabla f(x)\|_2^2 \geq \mu \left( f(x) - f(x^*) \right), \quad \forall x \in \mathbb{R}^d,
$$

where:

-   $\nabla f(x)$ is the gradient of $f$ at $x$,
-   $f(x^*)$ is the minimum value of $f(x)$ over $\mathbb{R}^d$,
-   $\mu$ is referred to as the **PL constant**.

The PL inequality is less restrictive than strong convexity and applies
to a broader class of functions. It guarantees that gradient-based
optimization methods converge to the minimum value under this condition.
This makes it a valuable property for analyzing optimization algorithms.
Here is an example demonstrating this case. The chosen function is neither convex nor concave but satisfies the PL inequality.

### An Example

```{r, fig.width=7, fig.height=5}
# Define the function f(x) = x^2 + 3 * sin^2(x)
f_value <- function(x) {
  return(x^2 + 3 * sin(x)^2)
}

#Initialize the parameters
initial_point <- 5
D <- 25
eigen_max <- 8

#Calling beta_NGD_optimum function
result <- beta_NGD_optimum(initial_point, D, eigen_max, epsilon=0.1,f_value)

# Print the result
cat("Optimal point found:", result$optimum, "\n")
cat("Function value at optimal point:", f_value(result$optimum), "\n")
f_array <- result$f_array

f_data <- data.frame(
  Index = 0:(length(f_array)-1),
  Value = f_array
)

library(ggplot2)
# Plot using ggplot2
print(ggplot(f_data, aes(x = Index, y = Value)) +
        geom_line(color = "blue") +           
        # geom_point(color = "red", size = 0.0001) + 
        labs(title = "function_value vs Iterations",
             x = "Iterations",
             y = "function value") +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5)))  


```

#### Observation

From the plot, we observe that the algorithm converges rapidly to the true optimum.

### General Non_Convex Setup(Neural Network Optimization)

In this section, we address the general non-convex setup, focusing on one of the most versatile and widely used frameworks: Neural Networks. For demonstration, we consider a two-layered neural network.

Consider a wide 2-layer neural network defined as $y = f_\theta(x)$,
where $\theta$ is the set of weights. The network uses sigmoid
activation functions. The output $y$ of this network represents the
preference or reward for the input $x$, which could be, for example, a
response generated by a large language model (LLM).

#### Hidden Loss Function

The underlying loss function is defined as:

$$
L(w) = \mathbb{E}_x \left[ (g(w; x) - y)^2 \right]
$$

This loss function is unknown to both the user and the learner. The
learner (LLM) does not have direct access to $L(w)$ or $f_\theta(x)$.
Instead, the user provides feedback in the form of **dueling
preferences**, defined as:

$$
1(L(w_1) > L(w_2))
$$

This binary feedback indicates whether the user prefers the response
generated by weights $w_1$ over $w_2$ on average.

$$
1(L(w_1, x) > L(w_2, x))
$$

Here, the preference is determined based on $x$ instead of the expected
loss.

#### Goal

The primary objective is to minimize the loss function $L(w)$ using only
the dueling feedback provided by the user. The learner then iteratively updates the weights 
$𝑤$ using this feedback, aiming to converge toward an optimal set of weights that minimizes the loss $L(w)$.

#### Getting Started (NN_optimization): A Simple Example

```{r plot-loss-iterations, fig.width=7, fig.height=5}
set.seed(324)
# Generate synthetic data
 n_samples <- 200
 n_features <- 6
 n_hidden <- 20

 #Generating the data matrix and weights
 X <- matrix(rnorm(n_samples * n_features), n_samples, n_features)
 true_weights <- list(
   W1 = matrix(runif(n_features * n_hidden), n_features, n_hidden),
   b1 = runif(n_hidden),
   W2 = matrix(runif(n_hidden), n_hidden, 1),
   b2 = runif(1)
 )
 hidden_layer <- 1 / (1 + exp(-(X %*% true_weights$W1 + true_weights$b1)))   #hidden layer
 output_layer <- hidden_layer %*% true_weights$W2 + true_weights$b2          #output layer
 y <- rowSums(output_layer)                                                  #generating y

 # Initialize neural network weights
 initial_weights <- list(
   W1 = matrix(rnorm(n_features * n_hidden), n_features, n_hidden),
   b1 = rep(0, n_hidden),
   W2 = matrix(rnorm(n_hidden), n_hidden, 1),
   b2 = 0
 )

 # Set optimization parameters
 eta <- 0.01
 gamma <- 0.1
 T <- 10000

 # Run optimization
 result <- NN_optimization(
   initial_weights = initial_weights,
   X = X,
   y = y,
   eta = eta,
   gamma = gamma,
   T = T
 )

library(ggplot2)

# Convert loss array to a data frame
loss_data <- data.frame(
  Iterations = seq_along(result$loss_array),
  Loss = result$loss_array
)

# Create the ggplot
ggplot(loss_data, aes(x = Iterations, y = Loss)) +
  geom_line(color = "blue") + 
  labs(
    title = "Loss Over Iterations",
    x = "Iterations",
    y = "Loss"
  ) +
  theme_classic() +  
  theme(
    plot.title = element_text(hjust = 0.5)  
  )

```

#### Observation

The loss is rapidly decreasing toward 0, demonstrating the effectiveness of our algorithm in this general non-convex optimization case. Even in a challenging setup like neural network optimization, the proposed method successfully converges, validating its robustness and applicability to non-convex scenarios.

### Conclusion

Our observations confirm that the original two cases presented in the paper are not confined to simple convex functions. From the plots, it is evident that the algorithms demonstrate strong performance, even in the non-convex case. This showcases the robustness and versatility of the methods, extending their applicability beyond convex optimization to more complex, real-world scenarios.

### Reference

If you use this package in your research, please cite the original
paper:

Saha, A., Koren, T., & Mansour, Y. (2021). Dueling Convex Optimization.
Proceedings of the 38th International Conference on Machine Learning.
