#(α, β)-Normalized Gradient Descent (NGD)
#This algorithm is particularly for α-strongly convex and β-smooth functions.

#Inputs
#x - initial starting point on R^d
#D - distance between x_1 & optimial x
#K_{\epsilon} - phase counts / the no of phases needed
#T_{\epsilon} - Query budget
#\eta - learning rate (step size) to control how much the point updates each iteration.


alpha_beta_NGD <- function(initial_point, alpha, beta, tolerance, max_iter = 1000) {


  # Main loop for phases
  for (k in 1:phase_count) {

  }

  return(x)
}

.

