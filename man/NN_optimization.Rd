% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NN_optimzation.R
\name{NN_optimization}
\alias{NN_optimization}
\title{Neural Network Optimization with Sigmoid activation}
\usage{
NN_optimization(initial_weights, X, y, eta, gamma, T)
}
\arguments{
\item{initial_weights}{A list containing the initial weights and biases of the neural network:
\itemize{
\item \code{W1}: A matrix of dimensions \code{n_features x n_hidden} representing input to hidden layer weights.
\emph{(n_features is the number of features in the input dataset, and n_hidden is the number of hidden layer neurons.)}
\item \code{b1}: A vector of size \code{n_hidden} representing biases for the hidden layer.
\item \code{W2}: A matrix of dimensions \code{n_hidden x n_output} representing hidden to output layer weights.
\emph{(n_output is typically 1 for regression problems or the number of classes for classification problems.)}
\item \code{b2}: A scalar representing the bias for the output layer.
}}

\item{X}{A numeric matrix of size \code{n_samples x n_features} representing input features.
\emph{(n_samples is the number of observations or rows in the dataset, and n_features is the number of columns or features in the input dataset.)}}

\item{y}{A numeric vector of size \code{n_samples} representing the response variable.
\emph{(n_samples should match the number of rows in \code{X}.)}}

\item{eta}{A numeric value representing the learning rate.}

\item{gamma}{A numeric value representing the perturbation parameter.}

\item{T}{An integer representing the maximum number of iterations for optimization.}
}
\value{
A list containing:
\itemize{
\item \code{weights}: A list of optimized weights and biases structured as the input \code{initial_weights}.
\item \code{loss_array}: A numeric vector containing the loss values over iterations.
}
}
\description{
This function optimizes a two-layer neural network with sigmoid activation using
normalized gradient descent.
}
\examples{
set.seed(567)

# Generate data
n_samples <- 100
n_features <- 5
n_hidden <- 10

#Generate design matrix
X <- matrix(rnorm(n_samples * n_features), n_samples, n_features)
true_weights <- list(
  W1 = matrix(runif(n_features * n_hidden), n_features, n_hidden),
  b1 = runif(n_hidden),
  W2 = matrix(runif(n_hidden), n_hidden, 1),
  b2 = runif(1)
)
#Sigmoid activation
hidden_layer <- 1 / (1 + exp(-(X \%*\% true_weights$W1 + true_weights$b1)))
output_layer <- hidden_layer \%*\% true_weights$W2 + true_weights$b2
# Generate y
y <- rowSums(output_layer)

# Initialize neural network weights
initial_weights <- list(
  W1 = matrix(rnorm(n_features * n_hidden), n_features, n_hidden),
  b1 = rep(0, n_hidden),
  W2 = matrix(rnorm(n_hidden), n_hidden, 1),
  b2 = 0
)

# Set optimization parameters
eta <- 0.01
gamma <- 0.1
T <- 10000

# Run optimization
result <- NN_optimization(
  initial_weights = initial_weights,
  X = X,
  y = y,
  eta = eta,
  gamma = gamma,
  T = T
)

library(ggplot2)
# Convert loss array to a data frame
loss_data <- data.frame(
 Iterations = seq_along(result$loss_array),
 Loss = result$loss_array
)
# Create the ggplot
ggplot(loss_data, aes(x = Iterations, y = Loss)) +
 geom_line(color = "blue") +
 labs(
   title = "Loss Over Iterations",
   x = "Iterations",
   y = "Loss"
 ) +
 theme_classic() +
 theme(
   plot.title = element_text(hjust = 0.5)
 )
}
