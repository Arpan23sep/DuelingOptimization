% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NN_optimzation.R
\name{NN_optimization}
\alias{NN_optimization}
\title{Neural Network Optimization with Sigmoid activation}
\usage{
NN_optimization(initial_weights, X, y, eta, gamma, T)
}
\arguments{
\item{initial_weights}{A list containing the initial weights and biases of the neural network:
\itemize{
\item \code{W1}: A matrix of dimensions \code{n_features x n_hidden} representing input to hidden layer weights.
\item \code{b1}: A vector of size \code{n_hidden} representing biases for the hidden layer.
\item \code{W2}: A matrix of dimensions \code{n_hidden x n_output} representing hidden to output layer weights.
\item \code{b2}: A scalar representing the bias for the output layer.
}}

\item{X}{A numeric matrix of size \code{n_samples x n_features} representing input features.}

\item{y}{A numeric vector of size \code{n_samples} representing the response variable.}

\item{eta}{A numeric value representing the learning rate.}

\item{gamma}{A numeric value representing the perturbation parameter.}

\item{T}{An integer representing the maximum number of iterations for optimization.}
}
\value{
A list containing:
\itemize{
\item \code{weights}: A list of optimized weights and biases structured as the input \code{initial_weights}.
\item \code{loss_array}: A numeric vector containing the loss values over iterations.
}
}
\description{
This function optimizes a two-layer neural network with sigmoid activation using
normalized gradient descent. The weights of the neural network are flattened for optimization
and converted back to their original structure after optimization.
}
\examples{
set.seed(42)

# Generate data
n_samples <- 100
n_features <- 5
n_hidden <- 10

#Generate design matrix
X <- matrix(rnorm(n_samples * n_features), n_samples, n_features)
true_weights <- list(
  W1 = matrix(runif(n_features * n_hidden), n_features, n_hidden),
  b1 = runif(n_hidden),
  W2 = matrix(runif(n_hidden), n_hidden, 1),
  b2 = runif(1)
)
hidden_layer <- 1 / (1 + exp(-(X \%*\% true_weights$W1 + true_weights$b1)))
output_layer <- hidden_layer \%*\% true_weights$W2 + true_weights$b2
# Generate y
y <- rowSums(output_layer)

# Initialize neural network weights
initial_weights <- list(
  W1 = matrix(rnorm(n_features * n_hidden), n_features, n_hidden),
  b1 = rep(0, n_hidden),
  W2 = matrix(rnorm(n_hidden), n_hidden, 1),
  b2 = 0
)

# Set optimization parameters
eta <- 0.01
gamma <- 0.1
T <- 10000

# Run optimization
result <- NN_optimization(
  initial_weights = initial_weights,
  X = X,
  y = y,
  eta = eta,
  gamma = gamma,
  T = T
)

# Plot the loss over iterations
plot(
 result$loss_array, type = "l",
 main = "Loss Over Iterations",
 xlab = "Iterations", ylab = "Loss"
)
}
