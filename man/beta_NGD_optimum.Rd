% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/beta_Normalized_Gradient.R
\name{beta_NGD_optimum}
\alias{beta_NGD_optimum}
\title{Optimized Normalized Gradient Descent with Parameter Tuning}
\usage{
beta_NGD_optimum(initial_point, D, eigen_max, epsilon = 0.1, f)
}
\arguments{
\item{initial_point}{A numeric vector representing the d-dimensional initial point.}

\item{D}{A numeric value representing the diameter of the search space.}

\item{eigen_max}{A numeric value representing the maximum eigenvalue of the Hessian (smoothness parameter).}

\item{epsilon}{A numeric value representing the desired optimization accuracy.(Default, epsilon = 0.01)}

\item{f}{A function representing the objective function to be minimized. This function
must take a numeric vector as input and return a numeric value.}
}
\value{
A list with the following components:
\itemize{
\item{optimum:} {A numeric vector representing the best point found.}
\item{f_array:} {A numeric vector containing the best function value observed at each iteration.}
}
}
\description{
Computes the parameters for normalized gradient descent based on the strong convexity
and smoothness properties of the objective function.
}
\examples{
# Logistic regression example with Beta_NGD_optimum
# Set random seed for reproducibility
set.seed(501)

# Generate data for logistic regression
n <- 100  # Number of data points
d <- 5     # Number of features
X <- matrix(rnorm(n * d), n, d)  # Generate random features

# Generate binary labels with a true beta
true_beta <- c(1, 2, 3, 4, 5)
prob <- 1 / (1 + exp(-X \%*\% true_beta))
y <- rbinom(n, 1, prob)  # Binary labels

# Define the logistic loss function as the objective function
f_value <- function(beta) {
  linear_comb <- X \%*\% beta
  loss <- -mean(y * log(1 / (1 + exp(-linear_comb))) +
                (1 - y) * log(1 - 1 / (1 + exp(-linear_comb))))
  return(loss)
}

# Initialize parameters
initial_point <- c(2, 1, 4, 3, 6)
D <- 10
Hessian <- t(X) \%*\% X      #Hessian for logistic regression
eigenvalues <- eigen(Hessian)$values
eigen_max <- 0.25 * max(eigenvalues)   #max eigenvalue of hessian

# Run the beta_NGD_optimum function
result <- beta_NGD_optimum(initial_point, D,
                           eigen_max, epsilon = 0.1, f_value)
point <- result$optimum
f_array <- result$f_array
print("Estimated parameters using beta_NGD_optimum:")
print(point)

# Compare with logistic solution from glm
# Fit logistic regression without intercept
glm_fit <- glm(y ~ X - 1, family = binomial)
print("GLM estimated beta:")
print(coef(glm_fit))

# Create a data frame for visualization
f_data <- data.frame(
  Index = 0:(length(f_array) - 1),
  Value = f_array
)

# Plot negative log-likelihood vs iterations using ggplot2
library(ggplot2)
ggplot(f_data, aes(x = Index, y = Value)) +
  geom_line(color = "blue") +
  labs(title = "Negative log-likelihood vs Iterations",
       x = "Iterations",
       y = "Negative log-likelihood") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
}
