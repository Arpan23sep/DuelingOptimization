% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/alpha_beta_NGD.R
\name{alpha_beta_NGD}
\alias{alpha_beta_NGD}
\title{(α, β)-Normalized Gradient Descent (NGD)}
\usage{
alpha_beta_NGD(initial_point, alpha, beta, D, tolerance = 0.1)
}
\arguments{
\item{initial_point}{A numeric vector representing the initial starting point in \eqn{R^d}.}

\item{alpha}{A numeric value representing the strong convexity parameter.}

\item{beta}{A numeric value representing the smoothness parameter.}

\item{tolerance}{A numeric value representing the desired accuracy for convergence.}
}
\value{
A numeric vector representing the approximate optimal point after performing
multiple phases of gradient descent.
}
\description{
This algorithm performs gradient descent on an α-strongly convex and β-smooth function.
It iteratively refines the solution over multiple phases, leveraging both the strong convexity
and smoothness properties to achieve faster convergence.
}
\details{
The (α, β)-Normalized Gradient Descent (NGD) algorithm is designed for functions that are
both α-strongly convex and β-smooth. The algorithm progresses over several phases, where
each phase has its own step size, perturbation parameter, and query budget. The algorithm
aims to reduce the initial distance from the optimal point to within a specified tolerance.
\itemize{
\item \strong{x}: Initial starting point in \eqn{R^d}.
\item \strong{D}: Initial distance between \eqn{x_1} and the optimal point.
\item \strong{K_\{\epsilon\}}: Number of phases needed for convergence.
\item \strong{T_\{\epsilon\}}: Query budget.
\item \strong{η}: Learning rate (step size) for controlling the update size at each iteration.
}
}
\examples{
initial_point <- c(0, 0)
alpha <- 0.1
beta <- 1.0
tolerance <- 0.01
result <- alpha_beta_NGD(initial_point, alpha, beta,2, tolerance)
print(result)
}
